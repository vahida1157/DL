{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx9SbT5jP6Ip",
        "outputId": "b79a08ca-d874-40f5-a077-0f1a30d36d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal values:\n",
            " tensor([0.4133, 0.4259, 0.4450, 0.4673, 0.4915, 0.5158, 0.5345, 0.5402, 0.4103,\n",
            "        0.4199, 0.4363, 0.4573, 0.4823, 0.5127, 0.5450, 0.5567, 0.3953, 0.3925,\n",
            "        0.3743, 0.0000, 0.4209, 0.4930, 0.5605, 0.5852, 0.3679, 0.3517, 0.3055,\n",
            "        0.1998, 0.3001, 0.0000, 0.5684, 0.6277, 0.3306, 0.2899, 0.1965, 0.0000,\n",
            "        0.2888, 0.3615, 0.5343, 0.6892, 0.3035, 0.0000, 0.0000, 0.0861, 0.2136,\n",
            "        0.2724, 0.0000, 0.7717, 0.2859, 0.0000, 0.0571, 0.0472, 0.0000, 0.2504,\n",
            "        0.0000, 0.8776, 0.2772, 0.1984, 0.1257, 0.0000, 0.2395, 0.4863, 0.7371,\n",
            "        0.0000])\n",
            "Optimal policy:\n",
            " tensor([3., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 2., 2., 1., 3., 3.,\n",
            "        0., 0., 2., 3., 2., 1., 3., 3., 3., 1., 0., 0., 2., 2., 0., 3., 0., 0.,\n",
            "        2., 1., 3., 2., 0., 0., 0., 1., 3., 0., 0., 2., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 2., 0., 1., 0., 0., 1., 2., 1., 0.])\n",
            "Average total reward under the optimal policy: 0.63575\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gym\n",
        "\n",
        "env = gym.make('FrozenLake-v0', map_name=\"8x8\")\n",
        "\n",
        "gamma = 0.99\n",
        "\n",
        "threshold = 0.0001\n",
        "\n",
        "\n",
        "def value_iteration(env, gamma, threshold):\n",
        "    n_state = env.observation_space.n\n",
        "    n_action = env.action_space.n\n",
        "    V = torch.zeros(n_state)\n",
        "    while True:\n",
        "        V_temp = torch.empty(n_state)\n",
        "        for state in range(n_state):\n",
        "            v_actions = torch.zeros(n_action)\n",
        "            for action in range(n_action):\n",
        "                for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
        "                    v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n",
        "            V_temp[state] = torch.max(v_actions)\n",
        "        max_delta = torch.max(torch.abs(V - V_temp))\n",
        "        V = V_temp.clone()\n",
        "        if max_delta <= threshold:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "\n",
        "def extract_optimal_policy(env, V_optimal, gamma):\n",
        "    n_state = env.observation_space.n\n",
        "    n_action = env.action_space.n\n",
        "    optimal_policy = torch.zeros(n_state)\n",
        "    for state in range(n_state):\n",
        "        v_actions = torch.zeros(n_action)\n",
        "        for action in range(n_action):\n",
        "            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
        "                v_actions[action] += trans_prob * (reward + gamma * V_optimal[new_state])\n",
        "        optimal_policy[state] = torch.argmax(v_actions)\n",
        "    return optimal_policy\n",
        "\n",
        "\n",
        "V_optimal = value_iteration(env, gamma, threshold)\n",
        "print('Optimal values:\\n', V_optimal)\n",
        "\n",
        "\n",
        "optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n",
        "print('Optimal policy:\\n', optimal_policy)\n",
        "\n",
        "\n",
        "def run_episode(env, policy):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "        action = policy[state].item()\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if is_done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "n_episode = 4000\n",
        "total_rewards = []\n",
        "for episode in range(n_episode):\n",
        "    total_reward = run_episode(env, optimal_policy)\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "print('Average total reward under the optimal policy:', sum(total_rewards) / n_episode)"
      ]
    }
  ]
}